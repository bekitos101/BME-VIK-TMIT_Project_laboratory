{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "9koeTQBp_0rR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTPoXlLe4uxK",
        "outputId": "4d858928-cdbb-4be9-b0a0-939adb2b9a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio transformers tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "40ss2vG64-iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otVcmeFy5IF6",
        "outputId": "fb88a005-0ccd-47dd-8e1d-f069c1885eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = Path(\"/content/drive/MyDrive/ProjectLabTMIT\")\n",
        "df_index = pd.read_csv(BASE_DIR / \"df_index_with_clip.csv\")"
      ],
      "metadata": {
        "id": "Jhje8Xrz5Pw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_index.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iufv0SwZ5c_I",
        "outputId": "e0164c42-dcbb-48c1-8686-a2a2794304cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          base_id      class  \\\n",
            "0  n02510455_4616  n02510455   \n",
            "1  n02510455_4616  n02510455   \n",
            "2  n02510455_4616  n02510455   \n",
            "3  n02510455_4616  n02510455   \n",
            "4  n02510455_4616  n02510455   \n",
            "\n",
            "                                            eeg_path  \\\n",
            "0  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "1  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "2  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "3  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "4  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "\n",
            "                                          image_path  \\\n",
            "0  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "1  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "2  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "3  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "4  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "\n",
            "                                        caption_path  \\\n",
            "0  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "1  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "2  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "3  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "4  /content/drive/MyDrive/capstone/images/n025104...   \n",
            "\n",
            "                                       clip_emb_path  \n",
            "0  /content/drive/MyDrive/ProjectLabTMIT/clip_emb...  \n",
            "1  /content/drive/MyDrive/ProjectLabTMIT/clip_emb...  \n",
            "2  /content/drive/MyDrive/ProjectLabTMIT/clip_emb...  \n",
            "3  /content/drive/MyDrive/ProjectLabTMIT/clip_emb...  \n",
            "4  /content/drive/MyDrive/ProjectLabTMIT/clip_emb...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Module\n",
        "\n"
      ],
      "metadata": {
        "id": "CSN8y2fe5r98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EEGDatasetV1(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.labels = sorted(df[\"class\"].unique())\n",
        "        self.label2id = {c: i for i, c in enumerate(self.labels)}\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        eeg_img = Image.open(row.eeg_path).convert(\"RGB\")\n",
        "        eeg = self.transform(eeg_img)\n",
        "\n",
        "        clip_emb = torch.tensor(np.load(row.clip_emb_path), dtype=torch.float32)\n",
        "        label = torch.tensor(self.label2id[row[\"class\"]], dtype=torch.long)\n",
        "\n",
        "        return eeg, clip_emb, label\n"
      ],
      "metadata": {
        "id": "r3E99cjn5nbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EEGDatasetV2(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.labels = sorted(df[\"class\"].unique())\n",
        "        self.label2id = {c: i for i, c in enumerate(self.labels)}\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        eeg_img = Image.open(row.eeg_path).convert(\"RGB\")\n",
        "        eeg = self.transform(eeg_img)\n",
        "\n",
        "        clip_emb = torch.tensor(np.load(row.clip_emb_path), dtype=torch.float32)\n",
        "        label = torch.tensor(self.label2id[row[\"class\"]], dtype=torch.long)\n",
        "\n",
        "        return eeg, clip_emb, label\n"
      ],
      "metadata": {
        "id": "YzYmqhLD5zpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGDatasetV3(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.labels = sorted(df[\"class\"].unique())\n",
        "        self.label2id = {c: i for i, c in enumerate(self.labels)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        eeg = Image.open(row.eeg_path).convert(\"L\")\n",
        "\n",
        "        eeg = eeg.resize((440, 128))   # (width, height)\n",
        "\n",
        "        eeg = np.array(eeg).astype(np.float32) / 255.0\n",
        "\n",
        "        eeg = eeg[np.newaxis, :, :]  # (1, H, W)\n",
        "\n",
        "        eeg_tensor = torch.tensor(eeg, dtype=torch.float32)\n",
        "\n",
        "        clip_emb = torch.tensor(np.load(row.clip_emb_path), dtype=torch.float32)\n",
        "        label = torch.tensor(self.label2id[row[\"class\"]], dtype=torch.long)\n",
        "\n",
        "        return eeg_tensor, clip_emb, label\n"
      ],
      "metadata": {
        "id": "ScYzAQ48531k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Module"
      ],
      "metadata": {
        "id": "fJyNJrUe6N1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline V1"
      ],
      "metadata": {
        "id": "LvRZF39RAblf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EEGEncoderV1(nn.Module):\n",
        "    def __init__(self, emb_dim=512, num_classes=40):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 224 -> 112\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 112 -> 56\n",
        "        )\n",
        "\n",
        "        self.fc_emb = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 56 * 56, emb_dim)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.features(x)\n",
        "        h = self.fc_emb(h)\n",
        "        h = F.normalize(h, dim=-1)\n",
        "        y = self.classifier(h)\n",
        "        return h, y\n"
      ],
      "metadata": {
        "id": "jgAhzMaj6KP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved Version - V2"
      ],
      "metadata": {
        "id": "ofhmvkNqAgNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer2D(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, padding, dilation):\n",
        "        super().__init__()\n",
        "        self.add_module(\"bn\", nn.BatchNorm2d(in_channels))\n",
        "        self.add_module(\"relu\", nn.ReLU(inplace=True))\n",
        "        self.add_module(\"conv\", nn.Conv2d(\n",
        "            in_channels, out_channels,\n",
        "            kernel_size=kernel,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation,\n",
        "            bias=True\n",
        "        ))\n"
      ],
      "metadata": {
        "id": "4OJXcjt16SA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dilation_list, kernel, stride):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        paddings = []\n",
        "        for dil in dilation_list:\n",
        "            k = kernel[1] * dil - 1\n",
        "            pad = math.floor(k / 2)\n",
        "            paddings.append((0, pad))\n",
        "\n",
        "        for pad, dil in zip(paddings, dilation_list):\n",
        "            layers.append(\n",
        "                ConvLayer2D(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel, stride,\n",
        "                    padding=pad,\n",
        "                    dilation=(1, dil)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = [layer(x) for layer in self.layers]\n",
        "        min_w = min(f.shape[-1] for f in feats)\n",
        "        feats = [f[..., :min_w] for f in feats]\n",
        "        return torch.cat(feats, dim=1)\n"
      ],
      "metadata": {
        "id": "JzpoiATe_tQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, height, n_layers=4):\n",
        "        super().__init__()\n",
        "\n",
        "        kernel_sizes = [32, 16, 8, 4]\n",
        "        layers = []\n",
        "\n",
        "        for k_h in kernel_sizes:\n",
        "            pad_h = k_h // 2\n",
        "            layers.append(\n",
        "                ConvLayer2D(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel=(k_h, 1),\n",
        "                    stride=(1,1),\n",
        "                    padding=(pad_h, 0),\n",
        "                    dilation=1\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = [layer(x) for layer in self.layers]\n",
        "        min_h = min(f.shape[-2] for f in feats)\n",
        "        min_w = min(f.shape[-1] for f in feats)\n",
        "        feats = [f[..., :min_h, :min_w] for f in feats]\n",
        "        return torch.cat(feats, dim=1)\n"
      ],
      "metadata": {
        "id": "t_RmWqiG_vGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(ch, ch, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        return self.relu(out + res)\n"
      ],
      "metadata": {
        "id": "HPvdvQJW_yEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ChannelNet\n",
        "\n"
      ],
      "metadata": {
        "id": "iPLtePmv6ZTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EEGEncoderV3(nn.Module):\n",
        "    def __init__(self, embedding_dim=512, num_classes=40,\n",
        "                 in_channels=1, height=128, width=440):\n",
        "        super().__init__()\n",
        "\n",
        "        self.temp = TemporalBlock(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=10,\n",
        "            dilation_list=[1, 2, 4, 8, 16],\n",
        "            kernel=(1, 33),\n",
        "            stride=(1, 2)\n",
        "        )\n",
        "\n",
        "        self.spatial = SpatialBlock(\n",
        "            in_channels=10 * 5,\n",
        "            out_channels=50,\n",
        "            height=height\n",
        "        )\n",
        "\n",
        "        res_in = 50 * 4\n",
        "        self.res_blocks = nn.ModuleList([ResidualBlock(res_in) for _ in range(4)])\n",
        "\n",
        "        self.down = ConvLayer2D(\n",
        "            res_in, 50,\n",
        "            kernel=3,\n",
        "            stride=2,\n",
        "            padding=1,\n",
        "            dilation=1\n",
        "        )\n",
        "\n",
        "        # Compute flattened size\n",
        "        x = torch.zeros(1, in_channels, height, width)\n",
        "        with torch.no_grad():\n",
        "            h = self.temp(x)\n",
        "            h = self.spatial(h)\n",
        "            for rb in self.res_blocks:\n",
        "                h = rb(h)\n",
        "            h = self.down(h)\n",
        "            flat_dim = h.view(1, -1).size(1)\n",
        "\n",
        "        self.embedding_proj = nn.Sequential(\n",
        "            nn.Linear(flat_dim, 1024),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(1024, embedding_dim)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.temp(x)\n",
        "        h = self.spatial(h)\n",
        "        for rb in self.res_blocks:\n",
        "            h = rb(h)\n",
        "        h = self.down(h)\n",
        "\n",
        "        h = h.view(h.size(0), -1)\n",
        "        emb = self.embedding_proj(h)\n",
        "        emb = F.normalize(emb, dim=-1)\n",
        "        cls = self.classifier(emb)\n",
        "        return emb, cls\n"
      ],
      "metadata": {
        "id": "bdD7b3Eb6V_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training module"
      ],
      "metadata": {
        "id": "Ya7uYZP_6nvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training modules of V1 and V2"
      ],
      "metadata": {
        "id": "eXTPEpiV6rj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_mse(model, dataloader, lr=1e-4, epochs=10, mse_w=0.5):\n",
        "    model = model.cuda()\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    mse = nn.MSELoss()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total = 0\n",
        "        for eeg, clip_emb, labels in dataloader:\n",
        "            eeg, clip_emb, labels = eeg.cuda(), clip_emb.cuda(), labels.cuda()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            emb_pred, cls_pred = model(eeg)\n",
        "            loss = (1 - mse_w) * mse(emb_pred, clip_emb) + mse_w * ce(cls_pred, labels)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "\n",
        "        print(f\"[MSE] Epoch {ep+1} Loss = {total/len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "s5-FFqv66f41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_loss(pred, target):\n",
        "    pred = F.normalize(pred, dim=-1)\n",
        "    target = F.normalize(target, dim=-1)\n",
        "    return 1 - (pred * target).sum(dim=-1).mean()"
      ],
      "metadata": {
        "id": "8s5mVfFC6pRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_eeg_encoder(model, dataloader, lr=1e-4, epochs=20, ce_weight=0.05):\n",
        "    model = model.cuda()\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        total = 0\n",
        "        model.train()\n",
        "\n",
        "        for eeg, clip_emb, labels in dataloader:\n",
        "            eeg = eeg.cuda()\n",
        "            clip_emb = clip_emb.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            opt.zero_grad()\n",
        "\n",
        "            emb_pred, cls_pred = model(eeg)\n",
        "            loss_emb = cosine_loss(emb_pred, clip_emb)\n",
        "            loss_cls = ce(cls_pred, labels)\n",
        "            loss = loss_emb + ce_weight * loss_cls\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "\n",
        "        print(f\"[COSINE] Epoch {ep+1}: Loss = {total/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "SRufY5Ha68HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_eeg_v3_optimized(\n",
        "    model,\n",
        "    dataloader,\n",
        "    lr=1e-4,\n",
        "    epochs=20,\n",
        "    ce_weight=0.05,\n",
        "    save_dir=\"/content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3\"\n",
        "):\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    device = \"cuda\"\n",
        "    model = model.to(device)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    scaler = GradScaler()   # for mixed precision\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {ep}/{epochs}\", leave=True)\n",
        "\n",
        "        for eeg, clip_emb, labels in loop:\n",
        "            eeg = eeg.to(device)\n",
        "            clip_emb = clip_emb.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # ---- FP16 AUTOCOMPUTE ----\n",
        "            with autocast():\n",
        "                emb_pred, cls_pred = model(eeg)\n",
        "\n",
        "                loss_emb = cosine_loss(emb_pred, clip_emb)\n",
        "                loss_cls = ce(cls_pred, labels)\n",
        "                loss = loss_emb + ce_weight * loss_cls\n",
        "\n",
        "            # ---- GRADIENT SCALING ----\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # ---- GRADIENT CLIPPING ----\n",
        "            scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            loop.set_postfix(loss=f\"{running_loss/len(loop):.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # ---- SAVE CHECKPOINT EACH EPOCH ----\n",
        "        ckpt_path = os.path.join(save_dir, f\"eeg_encoder_v3_epoch_{ep:02d}.pt\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "\n",
        "        print(f\"✔ Saved checkpoint: {ckpt_path}\\n\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "pPr5tjUFNyyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training scripts"
      ],
      "metadata": {
        "id": "9TOiEriA7ERr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Split"
      ],
      "metadata": {
        "id": "DsF3KRRt7Hc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "QrMik30K8C34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_index.copy()\n"
      ],
      "metadata": {
        "id": "w59G551g8FMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"class\"],\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "jpfz4kCG8Lnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n",
        "print(\"Unique classes:\", df[\"class\"].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFHFsNSj8PUm",
        "outputId": "d05d900d-c1c0-4c54-929c-e60b073f2c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 9572\n",
            "Val size: 2393\n",
            "Unique classes: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option A — Train Baseline V1"
      ],
      "metadata": {
        "id": "w8GrYDkT9ocZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = EEGDatasetV1(train_df)\n",
        "val_dataset   = EEGDatasetV1(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "model = EEGEncoderV1(emb_dim=512, num_classes=len(train_dataset.labels))\n",
        "train_with_mse(model, train_loader, epochs=10, mse_w=0.5)\n",
        "\n",
        "evaluate(model, val_loader)\n"
      ],
      "metadata": {
        "id": "6FEhEvOh7Dj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option B — ChannelNet V3"
      ],
      "metadata": {
        "id": "CuuDDcsT98G2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = EEGDatasetV3(train_df)\n",
        "val_dataset   = EEGDatasetV3(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "6n7cOH989xP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EEGEncoderV3(\n",
        "    embedding_dim=512,\n",
        "    num_classes=len(train_dataset.labels),\n",
        "    in_channels=1,\n",
        "    height=128,\n",
        "    width=440\n",
        ")\n",
        "\n",
        "trained_model = train_eeg_v3_optimized(\n",
        "    model,\n",
        "    train_loader,\n",
        "    lr=1e-4,\n",
        "    epochs=10,\n",
        "    ce_weight=0.05\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMCxv2RJOwiJ",
        "outputId": "0a1986cc-9998-41f7-a945-8d777b3c193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2511491712.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # for mixed precision\n",
            "Epoch 1/10:   0%|          | 0/599 [00:00<?, ?it/s]/tmp/ipython-input-2511491712.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/10: 100%|██████████| 599/599 [2:34:37<00:00, 15.49s/it, loss=0.3765]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_01.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 599/599 [03:24<00:00,  2.92it/s, loss=0.3711]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_02.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 599/599 [03:36<00:00,  2.77it/s, loss=0.3662]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_03.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 599/599 [03:49<00:00,  2.61it/s, loss=0.3608]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_04.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 599/599 [03:55<00:00,  2.54it/s, loss=0.3510]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_05.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 599/599 [04:02<00:00,  2.47it/s, loss=0.3324]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_06.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 599/599 [03:19<00:00,  3.01it/s, loss=0.3088]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_07.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 599/599 [03:02<00:00,  3.29it/s, loss=0.2889]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_08.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 599/599 [03:02<00:00,  3.29it/s, loss=0.2747]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_09.pt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 599/599 [03:01<00:00,  3.30it/s, loss=0.2676]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved checkpoint: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_10.pt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy = torch.zeros(1, 1, 128, 440).cuda()\n",
        "model = EEGEncoderV3().cuda()\n",
        "out = model(dummy)"
      ],
      "metadata": {
        "id": "4pjjIKnh-J1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "VyoYd0XJA5h4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_v3(model, loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for eeg, _, labels in loader:\n",
        "            eeg = eeg.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            _, logits = model(eeg)\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    acc = correct / total * 100\n",
        "    print(f\"Validation Accuracy: {acc:.2f}%  ({correct}/{total})\")\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "ag_M7w5OA_n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_ckpt = \"/content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_10.pt\"\n",
        "\n",
        "model = EEGEncoderV3(\n",
        "    embedding_dim=512,\n",
        "    num_classes=len(train_dataset.labels),\n",
        "    in_channels=1,\n",
        "    height=128,\n",
        "    width=440\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(best_ckpt, map_location=\"cuda\"))\n",
        "model = model.cuda().eval()\n",
        "\n",
        "print(\"Loaded:\", best_ckpt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y96OvNPVA4e-",
        "outputId": "af2b212a-7ab3-4121-fbb3-2876f374541f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: /content/drive/MyDrive/ProjectLabTMIT/checkpoints_v3/eeg_encoder_v3_epoch_10.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc = evaluate_v3(model, val_loader)\n",
        "val_acc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKpRCxCQCmQN",
        "outputId": "26caaac6-45d0-4479-8a26-01dab37feaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 11.95%  (286/2393)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.951525282072712"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_cosine_similarity(model, loader):\n",
        "    cos = nn.CosineSimilarity(dim=-1)\n",
        "    sims = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for eeg, clip, _ in loader:\n",
        "            eeg = eeg.cuda()\n",
        "            clip = clip.cuda()\n",
        "\n",
        "            emb_pred, _ = model(eeg)\n",
        "\n",
        "            s = cos(emb_pred, clip).mean().item()\n",
        "            sims.append(s)\n",
        "\n",
        "    return sum(sims) / len(sims)"
      ],
      "metadata": {
        "id": "evzSe-1FQ6mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sim = avg_cosine_similarity(model, val_loader)\n",
        "print(\"Average EEG→CLIP cosine similarity:\", sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li1v1QMBBD1i",
        "outputId": "e845cf10-58f5-48ed-cbe1-72fa6f32e60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average EEG→CLIP cosine similarity: 0.7726423684755961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tmHQZ32nQ-Kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}